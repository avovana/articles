Что решаем? Задачу "Top K".
Найти, к примеру, 10 самых упоминаемых слов в википедии.

====================================
1_big_map.py
====================================

Считать все слова в словарь с количеством упоминаний:
dict['арбуз'] += 1.
Затем вывести 10 слов с наибольшими упоминаниями.

Проблема:
Ограниченное кол-во байт в оперативной памяти(ОП) для хранения такого мега словаря.

Опустим тонкости использования оперативной памяти операционной системой и кодировки. Подсчитаем реальный пример. Допустим, есть 8 Гб ОП = 8589934592 байт. Слово "hello" занимает 5 байт. Это средняя длина слова в английском языке. Таких слов влезет 1,7 млрд = 1 717 986 918. Количество слов в англоязычной википедии 3,9 млрд. 

Уже мега словарь не влезает единомоментно в ОП. Представим далее, что и ОП меньше, и слов на входе еще больше.

Идея №2.
Считываем слова в словарь с инкрементом кол-ва упоминаний пока есть память. Выгружаем словарь в новый файл когда память кончилась:

====================================
2_devide_into_files_incremently.py
====================================
Проверка с объединением:
====================================
3_merge_files_to_check.py
====================================

Обнуляем словарь = освобождается ОП. 
Снова считываем слова... И так пока не вычитаем весь текст.

Далее нужно получить самые упоминаемые слова. 
Можно пройтись по всем файлам. Брать из каждого, к примеру, 33 очередные слова. Такая выборка из многих файлов уже может влезть в оперативку(картинка как докидываем в ОП).
Проблема: теряются слова, если они не оказались в текущем обрабатываемом диапазоне.
// картинка

Идея №3.
Сортировать файлы.
Сортируем файлы:
file1.txt:
aachen | 2
aachez | 1
aardvark | 5
zigzag | 3
К примеру, у нас 20 файлов. В каждом выполнили сортировку слов в алфавитном порядке. Тогда в каждом файле на техже самых уровнях окажутся примерно теже слова:
file19.txt:
aachen | 1
aachez | 2
aardv | 6
aardvark | 3
// спойлер Здесь, наверное, какая-то волшебная статистика самих текстов и частот появления определенных слов

Тогда мы можем вычитать сразу из всех файлов, к примеру, 3 первых слова.
file1.txt:
aachen | 2
aachez | 1
aardvark | 5

fileN.txt
...
file19.txt:
aachen | 1
aachez | 2
aardv | 6
Сложить повторяющиеся слова и получить итоговый файл:
file_of_looking_first_3_words_from_all_20_given_files.txt:
aachen | 3
aachez | 3
aardvark | 5
aardv | 6
Таким образом, после этого уже очень сильно уверены, что вычитали все слова "aachen", "aachez".

Далее читаем со всех файлов file1.txt ... file20.txt снова 3 очередные строки - с 4ой по 7ую.
Суммируем одинаковые слова. Получаем file_of_looking_second_3_words_from_all_20_given_files.txt.
Благодаря этому снова уменьшили число повторяющихся слов.
...
В итоге, получили уже меньшее кол-во файлов.
Снова проходимся по ним вычитывая по 3 строки и складывая значения повторяющихся слов и записывая результат в новый файл.
Так за сколько-то итераций мы схлопнем все повторяющиеся слова из всех файлов.


