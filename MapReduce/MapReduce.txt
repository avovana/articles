Что решаем? Задачу "Top K".
Найти, к примеру, 10 самых упоминаемых слов в википедии.

====================================
1_big_map.py
====================================

Считать все слова в словарь с количеством упоминаний:
dict['арбуз'] += 1.
Затем вывести 10 слов с наибольшими упоминаниями.

Проблема:
Ограниченное кол-во байт в оперативной памяти(ОП) для хранения такого мега словаря.

Опустим тонкости использования оперативной памяти операционной системой и кодировки. Подсчитаем реальный пример. Допустим, есть 8 Гб ОП = 8589934592 байт. Слово "hello" занимает 5 байт. Это средняя длина слова в английском языке. Таких слов влезет 1,7 млрд = 1 717 986 918. Количество слов в англоязычной википедии 3,9 млрд. 

Уже мега словарь не влезает единомоментно в ОП. Представим далее, что и ОП меньше, и слов на входе еще больше.

Идея №2.
Считываем слова в словарь с инкрементом кол-ва упоминаний пока есть память. Пускай у нас памяти - 2500 байт. Хватит для хранения максимум 2500/5=500 слов. Выгружаем словарь в новый файл(сначала=сверху будут самые популярные) когда память кончилась:
// Картинка с оперативкой
====================================
2_devide_into_files_incremently.py
====================================
Проверка с объединением:
====================================
3_merge_files_to_check.py
====================================

Обнуляем словарь = освобождается ОП. 
Снова считываем слова... И так пока не вычитаем весь текст.
// Дракон алгоритм

Далее нужно получить самые упоминаемые слова из всех файлов. Нужно проссумировать значения всех одинаковых слов из всех файлов.
Текущая идея - брать из каждого, к примеру, 33 слова. Такая выборка из многих файлов уже может влезть в оперативку:
15 файлов * 33 слова из каждого * 5 байт слово = 495 * 5 = 2475 байт. 2475 < 2500. proffit.
Итак:
Взяли первые 33 слова со всех файлов. Проссумировали значения одинаковых слов. Сделали итоговый файл1.
Взяли следующий 33 слова со всех файлов. Проссумировали значения одинаковых слов. Сделали еще итоговый файл2.
...
Итого:
В первом файле будут самые популярные слова по убывающей. Во втором чуть менее популярные слова. И т.д.

И всё хорошо, к примеру, для победиля конкурса - "the". Который стоит практически всегда на 1ом месте:
107 + 129 + 98 + 126 + 82 + ... = 1525
Во всяком случае в первых 33 словах во всех файлах он будет.

Но везёт не всем
// картинка с pick
Может быть подсчитано слово из данного диапазона. А потом оно встретится в следующем диапазон. Т.е. в 1ом файле мы не добрали всех количеств упоминаний.
Итого на самом деле:
Теряются упоминания/"очки побед" слова(а точнее размазываются по итоговым файлам), если слово одновременно не оказалось сразу во всех файлах в текущем обрабатываемом диапазоне.

Идея №3.
Сортировать файлы.
Сортируем файлы:
file1.txt:
aachen | 2
aachez | 1
aardvark | 5
zigzag | 3
К примеру, у нас 20 файлов. В каждом выполнили сортировку слов в алфавитном порядке. Тогда в каждом файле на техже самых уровнях окажутся примерно теже слова:
file19.txt:
aachen | 1
aachez | 2
aardv | 6
aardvark | 3
// спойлер Здесь, наверное, какая-то волшебная статистика самих текстов и частот появления определенных слов

Тогда мы можем вычитать сразу из всех файлов, к примеру, 3 первых слова.
file1.txt:
aachen | 2
aachez | 1
aardvark | 5

fileN.txt
...
file19.txt:
aachen | 1
aachez | 2
aardv | 6
Сложить повторяющиеся слова и получить итоговый файл:
file_of_looking_first_3_words_from_all_20_given_files.txt:
aachen | 3
aachez | 3
aardvark | 5
aardv | 6
Таким образом, после этого уже очень сильно уверены, что вычитали все слова "aachen", "aachez".

Далее читаем со всех файлов file1.txt ... file20.txt снова 3 очередные строки - с 4ой по 7ую.
Суммируем одинаковые слова. Получаем file_of_looking_second_3_words_from_all_20_given_files.txt.
Благодаря этому снова уменьшили число повторяющихся слов.
...
В итоге, получили уже меньшее кол-во файлов.
Снова проходимся по ним вычитывая по 3 строки и складывая значения повторяющихся слов и записывая результат в новый файл.
Так за сколько-то итераций мы схлопнем все повторяющиеся слова из всех файлов.


